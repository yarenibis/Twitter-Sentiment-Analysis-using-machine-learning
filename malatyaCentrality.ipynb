{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "aoBIdKSmZB93",
        "outputId": "e7ac04b5-2ea7-41f3-a9fd-d5c786ddb219"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d7d81c54-d2c8-4e39-9176-15423bc8f558\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d7d81c54-d2c8-4e39-9176-15423bc8f558\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
            "License(s): other\n",
            "Downloading sentiment140.zip to /content\n",
            " 96% 78.0M/80.9M [00:03<00:00, 31.3MB/s]\n",
            "100% 80.9M/80.9M [00:03<00:00, 24.3MB/s]\n",
            "Archive:  sentiment140.zip\n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Kaggle JSON'u Yükleme\n",
        "files.upload()\n",
        "\n",
        "# Kaggle JSON'u Doğru Konuma Taşıma\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Kaggle Veri Setini İndirme\n",
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "!unzip sentiment140.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNR6nZ-Vu0Bk",
        "outputId": "aad957dd-3b42-450f-b6f5-9a10f5101a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preprocessing data...\n",
            "Balancing data by sampling...\n",
            "Cleaning text data...\n",
            "Data loaded and preprocessed. Total rows: 144000\n",
            "Building similarity graph using KNN with chunking...\n",
            "Processed chunk 1/145\n",
            "Processed chunk 2/145\n",
            "Processed chunk 3/145\n",
            "Processed chunk 4/145\n",
            "Processed chunk 5/145\n",
            "Processed chunk 6/145\n",
            "Processed chunk 7/145\n",
            "Processed chunk 8/145\n",
            "Processed chunk 9/145\n",
            "Processed chunk 10/145\n",
            "Processed chunk 11/145\n",
            "Processed chunk 12/145\n",
            "Processed chunk 13/145\n",
            "Processed chunk 14/145\n",
            "Processed chunk 15/145\n",
            "Processed chunk 16/145\n",
            "Processed chunk 17/145\n",
            "Processed chunk 18/145\n",
            "Processed chunk 19/145\n",
            "Processed chunk 20/145\n",
            "Processed chunk 21/145\n",
            "Processed chunk 22/145\n",
            "Processed chunk 23/145\n",
            "Processed chunk 24/145\n",
            "Processed chunk 25/145\n",
            "Processed chunk 26/145\n",
            "Processed chunk 27/145\n",
            "Processed chunk 28/145\n",
            "Processed chunk 29/145\n",
            "Processed chunk 30/145\n",
            "Processed chunk 31/145\n",
            "Processed chunk 32/145\n",
            "Processed chunk 33/145\n",
            "Processed chunk 34/145\n",
            "Processed chunk 35/145\n",
            "Processed chunk 36/145\n",
            "Processed chunk 37/145\n",
            "Processed chunk 38/145\n",
            "Processed chunk 39/145\n",
            "Processed chunk 40/145\n",
            "Processed chunk 41/145\n",
            "Processed chunk 42/145\n",
            "Processed chunk 43/145\n",
            "Processed chunk 44/145\n",
            "Processed chunk 45/145\n",
            "Processed chunk 46/145\n",
            "Processed chunk 47/145\n",
            "Processed chunk 48/145\n",
            "Processed chunk 49/145\n",
            "Processed chunk 50/145\n",
            "Processed chunk 51/145\n",
            "Processed chunk 52/145\n",
            "Processed chunk 53/145\n",
            "Processed chunk 54/145\n",
            "Processed chunk 55/145\n",
            "Processed chunk 56/145\n",
            "Processed chunk 57/145\n",
            "Processed chunk 58/145\n",
            "Processed chunk 59/145\n",
            "Processed chunk 60/145\n",
            "Processed chunk 61/145\n",
            "Processed chunk 62/145\n",
            "Processed chunk 63/145\n",
            "Processed chunk 64/145\n",
            "Processed chunk 65/145\n",
            "Processed chunk 66/145\n",
            "Processed chunk 67/145\n",
            "Processed chunk 68/145\n",
            "Processed chunk 69/145\n",
            "Processed chunk 70/145\n",
            "Processed chunk 71/145\n",
            "Processed chunk 72/145\n",
            "Processed chunk 73/145\n",
            "Processed chunk 74/145\n",
            "Processed chunk 75/145\n",
            "Processed chunk 76/145\n",
            "Processed chunk 77/145\n",
            "Processed chunk 78/145\n",
            "Processed chunk 79/145\n",
            "Processed chunk 80/145\n",
            "Processed chunk 81/145\n",
            "Processed chunk 82/145\n",
            "Processed chunk 83/145\n",
            "Processed chunk 84/145\n",
            "Processed chunk 85/145\n",
            "Processed chunk 86/145\n",
            "Processed chunk 87/145\n",
            "Processed chunk 88/145\n",
            "Processed chunk 89/145\n",
            "Processed chunk 90/145\n",
            "Processed chunk 91/145\n",
            "Processed chunk 92/145\n",
            "Processed chunk 93/145\n",
            "Processed chunk 94/145\n",
            "Processed chunk 95/145\n",
            "Processed chunk 96/145\n",
            "Processed chunk 97/145\n",
            "Processed chunk 98/145\n",
            "Processed chunk 99/145\n",
            "Processed chunk 100/145\n",
            "Processed chunk 101/145\n",
            "Processed chunk 102/145\n",
            "Processed chunk 103/145\n",
            "Processed chunk 104/145\n",
            "Processed chunk 105/145\n",
            "Processed chunk 106/145\n",
            "Processed chunk 107/145\n",
            "Processed chunk 108/145\n",
            "Processed chunk 109/145\n",
            "Processed chunk 110/145\n",
            "Processed chunk 111/145\n",
            "Processed chunk 112/145\n",
            "Processed chunk 113/145\n",
            "Processed chunk 114/145\n",
            "Processed chunk 115/145\n",
            "Processed chunk 116/145\n",
            "Processed chunk 117/145\n",
            "Processed chunk 118/145\n",
            "Processed chunk 119/145\n",
            "Processed chunk 120/145\n",
            "Processed chunk 121/145\n",
            "Processed chunk 122/145\n",
            "Processed chunk 123/145\n",
            "Processed chunk 124/145\n",
            "Processed chunk 125/145\n",
            "Processed chunk 126/145\n",
            "Processed chunk 127/145\n",
            "Processed chunk 128/145\n",
            "Processed chunk 129/145\n",
            "Processed chunk 130/145\n",
            "Processed chunk 131/145\n",
            "Processed chunk 132/145\n",
            "Processed chunk 133/145\n",
            "Processed chunk 134/145\n",
            "Processed chunk 135/145\n",
            "Processed chunk 136/145\n",
            "Processed chunk 137/145\n",
            "Processed chunk 138/145\n",
            "Processed chunk 139/145\n",
            "Processed chunk 140/145\n",
            "Processed chunk 141/145\n",
            "Processed chunk 142/145\n",
            "Processed chunk 143/145\n",
            "Processed chunk 144/145\n",
            "Graph created with 144000 nodes and 1121931 edges.\n",
            "Calculating Malatya Centrality...\n",
            "Malatya Centrality calculated.\n",
            "Enter your tweets below (type 'exit' to stop):\n",
            "Enter a tweet: ı like it\n",
            "Predicting sentiment for the tweet: ı like it\n",
            "New node 144000 added to graph with 639 edges.\n",
            "Calculating Malatya Centrality...\n",
            "Malatya Centrality calculated.\n",
            "Positive centrality for the new tweet: 25341.28647576731\n",
            "Negative centrality for the new tweet: 19856.18122466911\n",
            "Predicted sentiment: Positive\n",
            "The sentiment of the tweet is: Positive\n",
            "Enter a tweet: ı dont understand anything\n",
            "Predicting sentiment for the tweet: ı dont understand anything\n",
            "New node 144001 added to graph with 128 edges.\n",
            "Calculating Malatya Centrality...\n",
            "Malatya Centrality calculated.\n",
            "Positive centrality for the new tweet: 1566.4094861372596\n",
            "Negative centrality for the new tweet: 3098.874194163713\n",
            "Predicted sentiment: Negative\n",
            "The sentiment of the tweet is: Negative\n",
            "Enter a tweet: humanity is corrupted\n",
            "Predicting sentiment for the tweet: humanity is corrupted\n",
            "New node 144002 added to graph with 0 edges.\n",
            "Calculating Malatya Centrality...\n",
            "Malatya Centrality calculated.\n",
            "Positive centrality for the new tweet: 0\n",
            "Negative centrality for the new tweet: 0\n",
            "Predicted sentiment: Negative\n",
            "The sentiment of the tweet is: Negative\n",
            "Enter a tweet: they are good people\n",
            "Predicting sentiment for the tweet: they are good people\n",
            "New node 144003 added to graph with 352 edges.\n",
            "Calculating Malatya Centrality...\n",
            "Malatya Centrality calculated.\n",
            "Positive centrality for the new tweet: 18988.527924692913\n",
            "Negative centrality for the new tweet: 9020.380413952495\n",
            "Predicted sentiment: Positive\n",
            "The sentiment of the tweet is: Positive\n",
            "Enter a tweet: stop it\n",
            "Predicting sentiment for the tweet: stop it\n",
            "New node 144004 added to graph with 250 edges.\n",
            "Calculating Malatya Centrality...\n",
            "Malatya Centrality calculated.\n",
            "Positive centrality for the new tweet: 1375.89490439395\n",
            "Negative centrality for the new tweet: 2574.7781272946368\n",
            "Predicted sentiment: Negative\n",
            "The sentiment of the tweet is: Negative\n",
            "Enter a tweet: exit\n",
            "Exiting...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1. Verilerin Yüklenmesi ve Ön İşlenmesi\n",
        "def load_and_preprocess_data(file_path, frac=0.1):\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    data = pd.read_csv(file_path, encoding='latin1', header=None)\n",
        "    data.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
        "\n",
        "    data = data[['target', 'text']]\n",
        "    data['sentiment'] = data['target'].apply(lambda x: 1 if x == 4 else 0)\n",
        "    data = data[['text', 'sentiment']]\n",
        "\n",
        "    print(\"Balancing data by sampling...\")\n",
        "    positive_tweets = data[data['sentiment'] == 1]\n",
        "    negative_tweets = data[data['sentiment'] == 0]\n",
        "\n",
        "    # Her sınıftan belirli bir oranda örnek al (%10)\n",
        "    positive_sample = positive_tweets.sample(frac=frac, random_state=42)\n",
        "    negative_sample = negative_tweets.sample(frac=frac, random_state=42)\n",
        "\n",
        "    # Dengeli veri kümesini birleştir\n",
        "    balanced_data = pd.concat([positive_sample, negative_sample]).sample(frac=0.3, random_state=42)\n",
        "\n",
        "    print(\"Cleaning text data...\")\n",
        "    balanced_data['cleaned_text'] = balanced_data['text'].str.replace(r\"http\\S+|www.\\S+\", \"\", regex=True)\n",
        "    balanced_data['cleaned_text'] = balanced_data['cleaned_text'].str.replace(r\"[^a-zA-Z\\s]\", \"\", regex=True).str.lower()\n",
        "\n",
        "    print(f\"Data loaded and preprocessed. Total rows: {len(balanced_data)}\")\n",
        "    return balanced_data\n",
        "\n",
        "# 2. KNN + Chunking ile Benzerlik Grafiği Oluşturma\n",
        "def build_similarity_graph_knn_chunking(data, vectorizer, chunk_size=500, n_neighbors=10, threshold=0.5):\n",
        "    print(\"Building similarity graph using KNN with chunking...\")\n",
        "    similarity_graph = nx.Graph()\n",
        "\n",
        "    tfidf_matrix = vectorizer.transform(data['cleaned_text'])\n",
        "\n",
        "    for chunk_idx, chunk_start in enumerate(range(0, tfidf_matrix.shape[0], chunk_size)):\n",
        "        chunk_end = min(chunk_start + chunk_size, tfidf_matrix.shape[0])\n",
        "        chunk_matrix = tfidf_matrix[chunk_start:chunk_end]\n",
        "\n",
        "        # KNN modeli oluştur ve komşuları bul\n",
        "        nn_model = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine').fit(tfidf_matrix)\n",
        "        distances, indices = nn_model.kneighbors(chunk_matrix)\n",
        "\n",
        "        for idx, neighbors in enumerate(indices):\n",
        "            global_idx = chunk_start + idx\n",
        "            similarity_graph.add_node(global_idx, sentiment=data.iloc[global_idx]['sentiment'])\n",
        "            for neighbor_idx, distance in zip(neighbors, distances[idx]):\n",
        "                similarity = 1 - distance  # Cosine distance -> similarity\n",
        "                if similarity > threshold:\n",
        "                    similarity_graph.add_edge(global_idx, neighbor_idx, weight=similarity)\n",
        "\n",
        "        print(f\"Processed chunk {chunk_idx + 1}/{tfidf_matrix.shape[0] // chunk_size + 1}\")\n",
        "\n",
        "    print(f\"Graph created with {similarity_graph.number_of_nodes()} nodes and {similarity_graph.number_of_edges()} edges.\")\n",
        "    return similarity_graph\n",
        "\n",
        "\n",
        "\n",
        "# 3. Malatya Centrality Hesaplama\n",
        "def calculate_malatya_centrality(graph):\n",
        "    print(\"Calculating Malatya Centrality...\")\n",
        "    centrality = {}\n",
        "    for node in graph.nodes:\n",
        "        neighbors = list(graph.neighbors(node))\n",
        "        if not neighbors:\n",
        "            centrality[node] = 0\n",
        "        else:\n",
        "            centrality[node] = sum(graph.degree[node] / graph.degree[n] for n in neighbors)\n",
        "    print(\"Malatya Centrality calculated.\")\n",
        "    return centrality\n",
        "\n",
        "def predict_sentiment(new_tweet, graph, vectorizer, tfidf_matrix, chunk_size=500, n_neighbors=10, threshold=0.5):\n",
        "    print(f\"Predicting sentiment for the tweet: {new_tweet}\")\n",
        "    new_tfidf = vectorizer.transform([new_tweet])\n",
        "\n",
        "    new_node_id = len(graph.nodes)\n",
        "    graph.add_node(new_node_id, text=new_tweet, sentiment=None)\n",
        "\n",
        "    for chunk_start in range(0, tfidf_matrix.shape[0], chunk_size):\n",
        "        chunk_end = min(chunk_start + chunk_size, tfidf_matrix.shape[0])\n",
        "        chunk_matrix = tfidf_matrix[chunk_start:chunk_end]\n",
        "\n",
        "        nn_model = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine').fit(chunk_matrix)\n",
        "        distances, indices = nn_model.kneighbors(new_tfidf)\n",
        "\n",
        "        for idx, distance in enumerate(distances[0]):\n",
        "            similarity = 1 - distance\n",
        "            global_idx = chunk_start + indices[0][idx]\n",
        "            if similarity > threshold:\n",
        "                graph.add_edge(new_node_id, global_idx, weight=similarity)\n",
        "\n",
        "    print(f\"New node {new_node_id} added to graph with {len(graph.edges(new_node_id))} edges.\")\n",
        "\n",
        "    # Malatya Centrality hesapla\n",
        "    centrality = calculate_malatya_centrality(graph)\n",
        "\n",
        "    positive_centrality = sum(\n",
        "        centrality[node] for node in graph.neighbors(new_node_id) if graph.nodes[node].get('sentiment') == 1\n",
        "    )\n",
        "    negative_centrality = sum(\n",
        "        centrality[node] for node in graph.neighbors(new_node_id) if graph.nodes[node].get('sentiment') == 0\n",
        "    )\n",
        "\n",
        "    print(f\"Positive centrality for the new tweet: {positive_centrality}\")\n",
        "    print(f\"Negative centrality for the new tweet: {negative_centrality}\")\n",
        "\n",
        "    result = \"Positive\" if positive_centrality > negative_centrality else \"Negative\"\n",
        "    print(f\"Predicted sentiment: {result}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "# 5. Ana Program\n",
        "if __name__ == \"__main__\":\n",
        "    # Verileri yükle ve örnekle\n",
        "    data = load_and_preprocess_data(\"training.1600000.processed.noemoticon.csv\", frac=0.3)\n",
        "\n",
        "    # TF-IDF vektörleştirici oluştur\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "    tfidf_matrix = vectorizer.fit_transform(data['cleaned_text'])\n",
        "\n",
        "    # KNN + Chunking ile benzerlik grafiği oluştur\n",
        "    similarity_graph = build_similarity_graph_knn_chunking(data, vectorizer, chunk_size=1000, n_neighbors=10, threshold=0.5)\n",
        "\n",
        "    # Malatya Centrality değerlerini hesapla\n",
        "    malatya_centrality = calculate_malatya_centrality(similarity_graph)\n",
        "    nx.set_node_attributes(similarity_graph, malatya_centrality, 'malatya_centrality')\n",
        "\n",
        "    # Kullanıcıdan tweet girişini al ve tahmin yap\n",
        "    print(\"Enter your tweets below (type 'exit' to stop):\")\n",
        "    while True:\n",
        "        new_tweet = input(\"Enter a tweet: \")\n",
        "        if new_tweet.lower() == 'exit':\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "        sentiment = predict_sentiment(new_tweet, similarity_graph, vectorizer, tfidf_matrix)\n",
        "        print(f\"The sentiment of the tweet is: {sentiment}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}